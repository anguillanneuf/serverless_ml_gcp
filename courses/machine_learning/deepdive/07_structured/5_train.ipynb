{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates distributed training and hyperparameter tuning on Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "  # copy canonical set of preprocessed files if you didn't do previous notebook\n",
    "  gsutil -m cp -R gs://cloud-training-demos/babyweight gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/babyweight/preproc/eval.csv-00000-of-00012\n",
      "gs://cloud-training-demos-ml/babyweight/preproc/train.csv-00000-of-00043\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TensorFlow code working on a subset of the data, we can package the TensorFlow code up as a Python module and train it on Cloud ML Engine.\n",
    "<p>\n",
    "<h2> Train on Cloud ML Engine </h2>\n",
    "<p>\n",
    "Training on Cloud ML Engine requires:\n",
    "<ol>\n",
    "<li> Making the code a Python package\n",
    "<li> Using gcloud to submit the training code to Cloud ML Engine\n",
    "</ol>\n",
    "<p>\n",
    "The code in model.py is the same as in the TensorFlow notebook. I just moved it to a file so that I could package it up as a module.\n",
    "(explore the <a href=\"babyweight/trainer\">directory structure</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def read_dataset(prefix, pattern, batch_size=512):\n",
      "def get_wide_deep():\n",
      "def serving_input_fn():\n",
      "def experiment_fn(output_dir):\n",
      "def train_and_evaluate(output_dir):\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "grep \"^def\" babyweight/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After moving the code to a package, make sure it works standalone. (Note the --pattern and --train_examples lines so that I am not trying to boil the ocean on my laptop). Even then, this takes about <b>a minute</b> in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf babyweight_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python -m trainer.task \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=babyweight_trained \\\n",
    "  --job-dir=./tmp \\\n",
    "  --pattern=\"00000-of-\" --train_examples=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code works in standalone mode, you can run it on Cloud ML Engine.  Because this is on the entire dataset, it will take a while. The training run took about <b> an hour </b> for me. You can monitor the job from the GCP console in the Cloud Machine Learning Engine section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/babyweight/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=1.4 \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, training finished, and the evaluation happened three times (filter in Stackdriver on the word \"dict\"):\n",
    "<pre>\n",
    "Saving dict for global step 390632: average_loss = 1.06578, global_step = 390632, loss = 545.55\n",
    "</pre>\n",
    "The final RMSE was 1.066 pounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 10437. Click <a href=\"/_proxy/56787/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10437"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/babyweight/trained_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 10437\n"
     ]
    }
   ],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameter tuning </h2>\n",
    "<p>\n",
    "All of these are command-line parameters to my program.  To do hyperparameter tuning, create hyperparam.xml and pass it as --configFile.\n",
    "This step will take <b>1 hour</b> -- you can increase maxParallelTrials or reduce maxTrials to get it done faster.  Since maxParallelTrials is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: average_loss\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 3\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: nembeds\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 30\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: nnsize\n",
    "      type: INTEGER\n",
    "      minValue: 64\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, you would hyper-parameter tune over your entire dataset, and not on a smaller subset (see --pattern). But because this is a demo, I wanted it to finish quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/hyperparam\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/babyweight/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=1.4 \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --pattern=\"00000-of-\" --train_examples=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud ml-engine jobs describe babyweight_180123_202458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Repeat training </h2>\n",
    "<p>\n",
    "This time with tuned parameters (note last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/babyweight/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=200000 --batch_size=35 --nembeds=16 --nnsize=281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
